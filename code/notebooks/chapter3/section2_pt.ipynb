{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (2.14.4)\n",
      "Requirement already satisfied: evaluate in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (4.32.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: filelock in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.12.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from transformers[sentencepiece]) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.3.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from transformers[sentencepiece]) (4.24.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to train a sequence classifier on one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/kaushik/projects/Transformers_Hugging_Face/.env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(params=model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MRPC (Microsoft Research Paraphrase Corpus) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 28.8k/28.8k [00:00<00:00, 32.6MB/s]\n",
      "Downloading metadata: 100%|██████████| 28.7k/28.7k [00:00<00:00, 28.2MB/s]\n",
      "Downloading readme: 100%|██████████| 27.9k/27.9k [00:00<00:00, 26.7MB/s]\n",
      "Downloading data: 6.22kB [00:00, 4.32MB/s]/3 [00:00<?, ?it/s]\n",
      "Downloading data: 1.05MB [00:00, 16.9MB/s]/3 [2:56:29<5:52:58, 10589.31s/it]\n",
      "Downloading data: 441kB [00:00, 20.1MB/s]2/3 [2:56:30<1:12:41, 4361.01s/it] \n",
      "Downloading data files: 100%|██████████| 3/3 [3:01:32<00:00, 3630.71s/it]  \n",
      "Generating train split: 100%|██████████| 3668/3668 [00:00<00:00, 21905.49 examples/s]\n",
      "Generating validation split: 100%|██████████| 408/408 [00:00<00:00, 7667.25 examples/s]\n",
      "Generating test split: 100%|██████████| 1725/1725 [00:00<00:00, 33996.37 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 1615 :: {'sentence1': 'However , Hayes , the CDC official , said there are many complicated interactions in play .', 'sentence2': 'But Hayes , of the CDC said , \" Many complicated interactions come into play that are often difficult to predict . \"', 'label': 0, 'idx': 1798}\n"
     ]
    }
   ],
   "source": [
    "# Access a random element\n",
    "import random\n",
    "\n",
    "idx = random.randint(0, len(raw_train_dataset)-1)\n",
    "print(f\"idx: {idx} :: {raw_train_dataset[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'However , EPA officials would not confirm the 20 percent figure .',\n",
       " 'sentence2': 'Only in the past few weeks have officials settled on the 20 percent figure .',\n",
       " 'label': 0,\n",
       " 'idx': 812}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_val_dataset = raw_datasets[\"validation\"]\n",
    "raw_val_dataset[87]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize first and second sentences separately. But we can't pass the two sequences to the model and get prediction. We need to handle the two sequences as a pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences_1 = tokenizer(raw_train_dataset[\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_train_dataset[\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode the ids back to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'one',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell would only work if we have enough RAM to store whole dataset during the tokenization, whereas datasets from the Datasets library can only keep the samples asked loaded in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3668/3668 [00:00<00:00, 3941.78 examples/s]\n",
      "Map: 100%|██████████| 408/408 [00:00<00:00, 9426.49 examples/s]\n",
      "Map: 100%|██████████| 1725/1725 [00:00<00:00, 17694.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab a few samples to batch them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': ['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       "  \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n",
       "  'They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',\n",
       "  'Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n",
       "  'The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .',\n",
       "  'Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier .',\n",
       "  'The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday .',\n",
       "  'The DVD-CCA then appealed to the state Supreme Court .'],\n",
       " 'sentence2': ['Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       "  \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\",\n",
       "  \"On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\",\n",
       "  'Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',\n",
       "  'PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .',\n",
       "  \"With the scandal hanging over Stewart 's company , revenue the first quarter of the year dropped 15 percent from the same period a year earlier .\",\n",
       "  'The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or 2.04 percent , to 1,520.15 .',\n",
       "  'The DVD CCA appealed that decision to the U.S. Supreme Court .'],\n",
       " 'label': [1, 0, 1, 0, 1, 1, 0, 1],\n",
       " 'idx': [0, 1, 2, 3, 4, 5, 6, 7],\n",
       " 'input_ids': [[101,\n",
       "   2572,\n",
       "   3217,\n",
       "   5831,\n",
       "   5496,\n",
       "   2010,\n",
       "   2567,\n",
       "   1010,\n",
       "   3183,\n",
       "   2002,\n",
       "   2170,\n",
       "   1000,\n",
       "   1996,\n",
       "   7409,\n",
       "   1000,\n",
       "   1010,\n",
       "   1997,\n",
       "   9969,\n",
       "   4487,\n",
       "   23809,\n",
       "   3436,\n",
       "   2010,\n",
       "   3350,\n",
       "   1012,\n",
       "   102,\n",
       "   7727,\n",
       "   2000,\n",
       "   2032,\n",
       "   2004,\n",
       "   2069,\n",
       "   1000,\n",
       "   1996,\n",
       "   7409,\n",
       "   1000,\n",
       "   1010,\n",
       "   2572,\n",
       "   3217,\n",
       "   5831,\n",
       "   5496,\n",
       "   2010,\n",
       "   2567,\n",
       "   1997,\n",
       "   9969,\n",
       "   4487,\n",
       "   23809,\n",
       "   3436,\n",
       "   2010,\n",
       "   3350,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   9805,\n",
       "   3540,\n",
       "   11514,\n",
       "   2050,\n",
       "   3079,\n",
       "   11282,\n",
       "   2243,\n",
       "   1005,\n",
       "   1055,\n",
       "   2077,\n",
       "   4855,\n",
       "   1996,\n",
       "   4677,\n",
       "   2000,\n",
       "   3647,\n",
       "   4576,\n",
       "   1999,\n",
       "   2687,\n",
       "   2005,\n",
       "   1002,\n",
       "   1016,\n",
       "   1012,\n",
       "   1019,\n",
       "   4551,\n",
       "   1012,\n",
       "   102,\n",
       "   9805,\n",
       "   3540,\n",
       "   11514,\n",
       "   2050,\n",
       "   4149,\n",
       "   11282,\n",
       "   2243,\n",
       "   1005,\n",
       "   1055,\n",
       "   1999,\n",
       "   2786,\n",
       "   2005,\n",
       "   1002,\n",
       "   6353,\n",
       "   2509,\n",
       "   2454,\n",
       "   1998,\n",
       "   2853,\n",
       "   2009,\n",
       "   2000,\n",
       "   3647,\n",
       "   4576,\n",
       "   2005,\n",
       "   1002,\n",
       "   1015,\n",
       "   1012,\n",
       "   1022,\n",
       "   4551,\n",
       "   1999,\n",
       "   2687,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2027,\n",
       "   2018,\n",
       "   2405,\n",
       "   2019,\n",
       "   15147,\n",
       "   2006,\n",
       "   1996,\n",
       "   4274,\n",
       "   2006,\n",
       "   2238,\n",
       "   2184,\n",
       "   1010,\n",
       "   5378,\n",
       "   1996,\n",
       "   6636,\n",
       "   2005,\n",
       "   5096,\n",
       "   1010,\n",
       "   2002,\n",
       "   2794,\n",
       "   1012,\n",
       "   102,\n",
       "   2006,\n",
       "   2238,\n",
       "   2184,\n",
       "   1010,\n",
       "   1996,\n",
       "   2911,\n",
       "   1005,\n",
       "   1055,\n",
       "   5608,\n",
       "   2018,\n",
       "   2405,\n",
       "   2019,\n",
       "   15147,\n",
       "   2006,\n",
       "   1996,\n",
       "   4274,\n",
       "   1010,\n",
       "   5378,\n",
       "   1996,\n",
       "   14792,\n",
       "   2005,\n",
       "   5096,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2105,\n",
       "   6021,\n",
       "   19481,\n",
       "   13938,\n",
       "   2102,\n",
       "   1010,\n",
       "   21628,\n",
       "   6661,\n",
       "   2020,\n",
       "   2039,\n",
       "   2539,\n",
       "   16653,\n",
       "   1010,\n",
       "   2030,\n",
       "   1018,\n",
       "   1012,\n",
       "   1018,\n",
       "   1003,\n",
       "   1010,\n",
       "   2012,\n",
       "   1037,\n",
       "   1002,\n",
       "   1018,\n",
       "   1012,\n",
       "   5179,\n",
       "   1010,\n",
       "   2383,\n",
       "   3041,\n",
       "   2275,\n",
       "   1037,\n",
       "   2501,\n",
       "   2152,\n",
       "   1997,\n",
       "   1037,\n",
       "   1002,\n",
       "   1018,\n",
       "   1012,\n",
       "   5401,\n",
       "   1012,\n",
       "   102,\n",
       "   21628,\n",
       "   6661,\n",
       "   5598,\n",
       "   2322,\n",
       "   16653,\n",
       "   1010,\n",
       "   2030,\n",
       "   1018,\n",
       "   1012,\n",
       "   1020,\n",
       "   1003,\n",
       "   1010,\n",
       "   2000,\n",
       "   2275,\n",
       "   1037,\n",
       "   2501,\n",
       "   5494,\n",
       "   2152,\n",
       "   2012,\n",
       "   1037,\n",
       "   1002,\n",
       "   1018,\n",
       "   1012,\n",
       "   5401,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   1996,\n",
       "   4518,\n",
       "   3123,\n",
       "   1002,\n",
       "   1016,\n",
       "   1012,\n",
       "   2340,\n",
       "   1010,\n",
       "   2030,\n",
       "   2055,\n",
       "   2340,\n",
       "   3867,\n",
       "   1010,\n",
       "   2000,\n",
       "   2485,\n",
       "   5958,\n",
       "   2012,\n",
       "   1002,\n",
       "   2538,\n",
       "   1012,\n",
       "   4868,\n",
       "   2006,\n",
       "   1996,\n",
       "   2047,\n",
       "   2259,\n",
       "   4518,\n",
       "   3863,\n",
       "   1012,\n",
       "   102,\n",
       "   18720,\n",
       "   1004,\n",
       "   1041,\n",
       "   13058,\n",
       "   1012,\n",
       "   6661,\n",
       "   5598,\n",
       "   1002,\n",
       "   1015,\n",
       "   1012,\n",
       "   6191,\n",
       "   2030,\n",
       "   1022,\n",
       "   3867,\n",
       "   2000,\n",
       "   1002,\n",
       "   2538,\n",
       "   1012,\n",
       "   6021,\n",
       "   2006,\n",
       "   1996,\n",
       "   2047,\n",
       "   2259,\n",
       "   4518,\n",
       "   3863,\n",
       "   2006,\n",
       "   5958,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   6599,\n",
       "   1999,\n",
       "   1996,\n",
       "   2034,\n",
       "   4284,\n",
       "   1997,\n",
       "   1996,\n",
       "   2095,\n",
       "   3333,\n",
       "   2321,\n",
       "   3867,\n",
       "   2013,\n",
       "   1996,\n",
       "   2168,\n",
       "   2558,\n",
       "   1037,\n",
       "   2095,\n",
       "   3041,\n",
       "   1012,\n",
       "   102,\n",
       "   2007,\n",
       "   1996,\n",
       "   9446,\n",
       "   5689,\n",
       "   2058,\n",
       "   5954,\n",
       "   1005,\n",
       "   1055,\n",
       "   2194,\n",
       "   1010,\n",
       "   6599,\n",
       "   1996,\n",
       "   2034,\n",
       "   4284,\n",
       "   1997,\n",
       "   1996,\n",
       "   2095,\n",
       "   3333,\n",
       "   2321,\n",
       "   3867,\n",
       "   2013,\n",
       "   1996,\n",
       "   2168,\n",
       "   2558,\n",
       "   1037,\n",
       "   2095,\n",
       "   3041,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   1996,\n",
       "   17235,\n",
       "   2850,\n",
       "   4160,\n",
       "   2018,\n",
       "   1037,\n",
       "   4882,\n",
       "   5114,\n",
       "   1997,\n",
       "   2459,\n",
       "   1012,\n",
       "   2676,\n",
       "   1010,\n",
       "   2030,\n",
       "   1015,\n",
       "   1012,\n",
       "   1016,\n",
       "   3867,\n",
       "   1010,\n",
       "   5494,\n",
       "   2012,\n",
       "   1015,\n",
       "   1010,\n",
       "   19611,\n",
       "   1012,\n",
       "   2321,\n",
       "   2006,\n",
       "   5958,\n",
       "   1012,\n",
       "   102,\n",
       "   1996,\n",
       "   6627,\n",
       "   1011,\n",
       "   17958,\n",
       "   17235,\n",
       "   2850,\n",
       "   4160,\n",
       "   12490,\n",
       "   1012,\n",
       "   11814,\n",
       "   2594,\n",
       "   24356,\n",
       "   2382,\n",
       "   1012,\n",
       "   4805,\n",
       "   2685,\n",
       "   1010,\n",
       "   2030,\n",
       "   1016,\n",
       "   1012,\n",
       "   5840,\n",
       "   3867,\n",
       "   1010,\n",
       "   2000,\n",
       "   1015,\n",
       "   1010,\n",
       "   19611,\n",
       "   1012,\n",
       "   2321,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   1996,\n",
       "   4966,\n",
       "   1011,\n",
       "   10507,\n",
       "   2050,\n",
       "   2059,\n",
       "   12068,\n",
       "   2000,\n",
       "   1996,\n",
       "   2110,\n",
       "   4259,\n",
       "   2457,\n",
       "   1012,\n",
       "   102,\n",
       "   1996,\n",
       "   4966,\n",
       "   10507,\n",
       "   2050,\n",
       "   12068,\n",
       "   2008,\n",
       "   3247,\n",
       "   2000,\n",
       "   1996,\n",
       "   1057,\n",
       "   1012,\n",
       "   1055,\n",
       "   1012,\n",
       "   4259,\n",
       "   2457,\n",
       "   1012,\n",
       "   102]],\n",
       " 'token_type_ids': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "# Length of samples\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create batch from the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
       "          2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
       "          3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
       "          1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
       "          2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  9805,  3540, 11514,  2050,  3079, 11282,  2243,  1005,  1055,\n",
       "          2077,  4855,  1996,  4677,  2000,  3647,  4576,  1999,  2687,  2005,\n",
       "          1002,  1016,  1012,  1019,  4551,  1012,   102,  9805,  3540, 11514,\n",
       "          2050,  4149, 11282,  2243,  1005,  1055,  1999,  2786,  2005,  1002,\n",
       "          6353,  2509,  2454,  1998,  2853,  2009,  2000,  3647,  4576,  2005,\n",
       "          1002,  1015,  1012,  1022,  4551,  1999,  2687,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2027,  2018,  2405,  2019, 15147,  2006,  1996,  4274,  2006,\n",
       "          2238,  2184,  1010,  5378,  1996,  6636,  2005,  5096,  1010,  2002,\n",
       "          2794,  1012,   102,  2006,  2238,  2184,  1010,  1996,  2911,  1005,\n",
       "          1055,  5608,  2018,  2405,  2019, 15147,  2006,  1996,  4274,  1010,\n",
       "          5378,  1996, 14792,  2005,  5096,  1012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2105,  6021, 19481, 13938,  2102,  1010, 21628,  6661,  2020,\n",
       "          2039,  2539, 16653,  1010,  2030,  1018,  1012,  1018,  1003,  1010,\n",
       "          2012,  1037,  1002,  1018,  1012,  5179,  1010,  2383,  3041,  2275,\n",
       "          1037,  2501,  2152,  1997,  1037,  1002,  1018,  1012,  5401,  1012,\n",
       "           102, 21628,  6661,  5598,  2322, 16653,  1010,  2030,  1018,  1012,\n",
       "          1020,  1003,  1010,  2000,  2275,  1037,  2501,  5494,  2152,  2012,\n",
       "          1037,  1002,  1018,  1012,  5401,  1012,   102],\n",
       "        [  101,  1996,  4518,  3123,  1002,  1016,  1012,  2340,  1010,  2030,\n",
       "          2055,  2340,  3867,  1010,  2000,  2485,  5958,  2012,  1002,  2538,\n",
       "          1012,  4868,  2006,  1996,  2047,  2259,  4518,  3863,  1012,   102,\n",
       "         18720,  1004,  1041, 13058,  1012,  6661,  5598,  1002,  1015,  1012,\n",
       "          6191,  2030,  1022,  3867,  2000,  1002,  2538,  1012,  6021,  2006,\n",
       "          1996,  2047,  2259,  4518,  3863,  2006,  5958,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  6599,  1999,  1996,  2034,  4284,  1997,  1996,  2095,  3333,\n",
       "          2321,  3867,  2013,  1996,  2168,  2558,  1037,  2095,  3041,  1012,\n",
       "           102,  2007,  1996,  9446,  5689,  2058,  5954,  1005,  1055,  2194,\n",
       "          1010,  6599,  1996,  2034,  4284,  1997,  1996,  2095,  3333,  2321,\n",
       "          3867,  2013,  1996,  2168,  2558,  1037,  2095,  3041,  1012,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1996, 17235,  2850,  4160,  2018,  1037,  4882,  5114,  1997,\n",
       "          2459,  1012,  2676,  1010,  2030,  1015,  1012,  1016,  3867,  1010,\n",
       "          5494,  2012,  1015,  1010, 19611,  1012,  2321,  2006,  5958,  1012,\n",
       "           102,  1996,  6627,  1011, 17958, 17235,  2850,  4160, 12490,  1012,\n",
       "         11814,  2594, 24356,  2382,  1012,  4805,  2685,  1010,  2030,  1016,\n",
       "          1012,  5840,  3867,  1010,  2000,  1015,  1010, 19611,  1012,  2321,\n",
       "          1012,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  1996,  4966,  1011, 10507,  2050,  2059, 12068,  2000,  1996,\n",
       "          2110,  4259,  2457,  1012,   102,  1996,  4966, 10507,  2050, 12068,\n",
       "          2008,  3247,  2000,  1996,  1057,  1012,  1055,  1012,  4259,  2457,\n",
       "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 0, 1, 0, 1, 1, 0, 1])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Processing the data (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
